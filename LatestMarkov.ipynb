{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_df = pd.read_csv('movie_conversations.tsv',encoding='ISO-8859-2', sep='\\t',warn_bad_lines =False,header=None)\n",
    "lines_df = pd.read_csv('movie_lines.tsv',sep='\\t',error_bad_lines=False,warn_bad_lines =False,header=None)\n",
    "\n",
    "characters_df = pd.read_csv('movie_characters_metadata.tsv',sep='\\t',warn_bad_lines =False,error_bad_lines=False,header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lines dataset : \")\n",
    "lines_df.columns = ['lines', 'charId','movieId', 'charName', 'dialogue']\n",
    "lines_df.dropna(how='any', inplace=True)\n",
    "lines_df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Characters Dataset : \\n\")\n",
    "characters_df.columns=['charId','charName','movieId','movieName','gender','positionInCredits']\n",
    "characters_df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(lines_df, characters_df, how='inner', on=['charId','movieId','charName'],\n",
    "         left_index=False, right_index=False, sort=True,\n",
    "         suffixes=('_x', '_y'), copy=True, indicator=False)\n",
    "merged_df.dropna(how='any', inplace=True)\n",
    "merged_df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Character senteences extraction\n",
    "from itertools import compress\n",
    "\n",
    "def getDialogues(merged_df, char_name):\n",
    "    # get list of booleans corresponding to the Character\n",
    "    char_line_bool = (merged_df[['charName']] == char_name)\n",
    "    char_line_bool = char_line_bool['charName'].tolist()\n",
    "    char_lines = list(compress(merged_df['dialogue'], char_line_bool))\n",
    "    return char_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_dialogues = getDialogues(merged_df, char_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "char_sentences = char_dialogues\n",
    "print(\"\\n\\n\\n\\n\")\n",
    "print(char_sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "s0 = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
    "tknzr.tokenize(s0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tag_sentence = []\n",
    "word_tag_sentence = []\n",
    "world_tags = []\n",
    "char_pos_tag_sentence = []\n",
    "char_word_tag_sentence = []\n",
    "char_world_tags = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(sentences).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(char_sentences).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tags for the whole corpus sentence wise --> list of lists\n",
    "\n",
    "for sentence in sentences[:]:\n",
    "    _text = tknzr.tokenize(sentence)\n",
    "    word_tags = nltk.pos_tag(_text)\n",
    "    word_tags = list(word_tags)\n",
    "    test1=[]\n",
    "    for tag in word_tags:\n",
    "        tag = list(tag)\n",
    "        test1.append(tag[1])\n",
    "    pos_tag_sentence.append(test1)\n",
    "\n",
    "pos_tag_sentence[:2]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character POS tags for the whole corpus sentence wise --> list of lists\n",
    "\n",
    "for sentence in char_sentences[:]:\n",
    "    _text = tknzr.tokenize(sentence)\n",
    "    char_word_tags = nltk.pos_tag(_text)\n",
    "    char_word_tags = list(char_word_tags)\n",
    "    char_test1=[]\n",
    "    for tag in char_word_tags:\n",
    "        tag = list(tag)\n",
    "        char_test1.append(tag[1])\n",
    "    char_pos_tag_sentence.append(char_test1)\n",
    "    \n",
    "char_pos_tag_sentence[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words token sentence wise for whole corpus --> list of lists\n",
    "\n",
    "for sentence in sentences[:]:\n",
    "    _text = tknzr.tokenize(sentence)\n",
    "    word_tags = nltk.pos_tag(_text)\n",
    "    word_tags = list(word_tags)\n",
    "    world_tags += word_tags\n",
    "    test2=[]\n",
    "    for tag in word_tags:\n",
    "        tag = list(tag)\n",
    "        test2.append(tag[0])\n",
    "    word_tag_sentence.append(test2)\n",
    "\n",
    "word_tag_sentence[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in char_sentences[:]:\n",
    "    _text = tknzr.tokenize(sentence)\n",
    "    char_word_tags = nltk.pos_tag(_text)\n",
    "    char_word_tags = list(char_word_tags)\n",
    "    char_world_tags += char_word_tags\n",
    "    char_test2=[]\n",
    "    for tag in char_word_tags:\n",
    "        tag = list(tag)\n",
    "        char_test2.append(tag[0])\n",
    "    char_word_tag_sentence.append(char_test2)\n",
    "\n",
    "char_word_tag_sentence[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_arr_tags =[]\n",
    "final_arr_words = []\n",
    "some_arr1 = []\n",
    "some_arr2 = []\n",
    "char_final_arr_tags =[]\n",
    "char_final_arr_words = []\n",
    "char_some_arr1 = []\n",
    "char_some_arr2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "def make_ngrams(sentence):\n",
    "    ngrams2 = list(ngrams(sentence, 2))\n",
    "    return ngrams2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for tag_sentence in pos_tag_sentence:\n",
    "    some_arr1 = make_ngrams(tag_sentence)\n",
    "    final_arr_tags.append(some_arr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag_sentence in char_pos_tag_sentence:\n",
    "    char_some_arr1 = make_ngrams(tag_sentence)\n",
    "    char_final_arr_tags.append(make_ngrams(tag_sentence))  #char_some_arr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for tag_sentence in word_tag_sentence:\n",
    "    some_arr2 = make_ngrams(tag_sentence)\n",
    "    final_arr_words.append(some_arr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag_sentence in char_word_tag_sentence:\n",
    "    char_some_arr2 = make_ngrams(tag_sentence)\n",
    "    char_final_arr_words.append(char_some_arr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_arr_tags[:1], len(final_arr_tags) # bigrams of POS ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_arr_words[:1], len(final_arr_words) # Bigrams of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_final_arr_tags[:1], len(char_final_arr_tags) # Bigrams of Characteer's POS tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_final_arr_words[:1], len(char_final_arr_words) # Bigrams of Character's Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "flatten_word_grams = reduce(lambda x,y :x+y ,final_arr_words[:])\n",
    "\n",
    "flaten_tag_grams = reduce(lambda x,y :x+y ,final_arr_tags[:])\n",
    "\n",
    "char_flatten_word_grams = reduce(lambda x,y :x+y ,char_final_arr_words[:])\n",
    "\n",
    "char_flaten_tag_grams = reduce(lambda x,y :x+y ,char_final_arr_tags[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqDist_tag_grams = nltk.FreqDist(flaten_tag_grams[:]) # Bigrams of POS tags for corpora\n",
    "freqDist_word_grams = nltk.FreqDist(flatten_word_grams[:]) # Bigrams of words for corpora\n",
    "freqDist_World_word_grams = nltk.FreqDist(world_tags[:]) # Bigram of word and POS tag\n",
    "\n",
    "char_freqDist_tag_grams = nltk.FreqDist(char_flaten_tag_grams[:]) # Character Bigrams of POS tags for corpora\n",
    "char_freqDist_word_grams = nltk.FreqDist(char_flatten_word_grams[:]) # Character Bigrams of words for corpora\n",
    "char_freqDist_World_word_grams = nltk.FreqDist(char_world_tags[:]) # Character Bigram of word and POS tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save(\"freqDist_tag_grams.npy\", freqDist_tag_grams)\n",
    "np.save(\"freqDist_word_grams.npy\", freqDist_word_grams)\n",
    "np.save(\"freqDist_World_word_grams.npy\", freqDist_World_word_grams)\n",
    "np.save(\"char_freqDist_tag_grams.npy\", char_freqDist_tag_grams)\n",
    "np.save(\"char_freqDist_word_grams.npy\", char_freqDist_word_grams)\n",
    "np.save(\"char_freqDist_World_word_grams.npy\", char_freqDist_World_word_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First search in character tags and then in the general corpora\n",
    "\n",
    "def find_best_fit_word(input_word): ## char and the world tags \n",
    "    top_successor_tags = []\n",
    "    top_successor_words = []\n",
    "\n",
    "    tag_of_current_word = [word_tag_tuple[1] for (word_tag_tuple,frequency) in freqDist_World_word_grams.most_common() if word_tag_tuple[0] == input_word]\n",
    "    char_tag_of_current_word = [word_tag_tuple[1] for (word_tag_tuple,frequency) in char_freqDist_World_word_grams.most_common() if word_tag_tuple[0] == input_word]\n",
    "\n",
    "    most_freq_successor_for_word = [wt[1] for (wt, _) in freqDist_word_grams.most_common() if wt[0] == input_word]\n",
    "    char_most_freq_successor_for_word = [wt[1] for (wt, _) in char_freqDist_word_grams.most_common() if wt[0] == input_word]\n",
    "\n",
    "    most_freq_successor_for_tag = [wt[1] for (wt, _) in freqDist_tag_grams.most_common() if wt[0] in tag_of_current_word]\n",
    "    char_most_freq_successor_for_tag = [wt[1] for (wt, _) in char_freqDist_tag_grams.most_common() if wt[0] in tag_of_current_word]\n",
    "\n",
    "    _top_successor_words = most_freq_successor_for_word[:5]\n",
    "    _top_successor_tags = most_freq_successor_for_tag[:5]\n",
    "    _char_top_successor_words = char_most_freq_successor_for_word[:5]\n",
    "    _char_top_successor_tags = char_most_freq_successor_for_tag[:5]    \n",
    "    if not _char_top_successor_words:\n",
    "        top_successor_words = _top_successor_words\n",
    "    \n",
    "    elif not _char_top_successor_tags:\n",
    "        top_successor_tags = _top_successor_tags\n",
    "    else:\n",
    "        top_successor_words = list(set(_top_successor_words + _char_top_successor_words))\n",
    "        top_successor_tags = list(set(_top_successor_tags + _char_top_successor_tags))\n",
    "    \n",
    "    final_list = [(value,freq) for (value, freq) in freqDist_World_word_grams.most_common() if value[0] in top_successor_words and value[1] in top_successor_tags]\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_sentence(input_word):\n",
    "    import random\n",
    "    \n",
    "    output_sentence = \"\"\n",
    "\n",
    "    output_sentence += input_word\n",
    "    try:\n",
    "        best_fit_gram = find_best_fit_word(input_word)\n",
    "        best_fit_choice = random.choice(best_fit_gram)\n",
    "        best_fit_word = best_fit_choice[0][0]\n",
    "        output_sentence = output_sentence + \" \" + best_fit_word\n",
    "        while best_fit_word != '.':\n",
    "            best_fit_gram = find_best_fit_word(best_fit_word)\n",
    "            best_fit_choice = random.choice(best_fit_gram)\n",
    "            best_fit_word = best_fit_choice[0][0]\n",
    "            output_sentence = output_sentence + \" \" + best_fit_word\n",
    "    except IndexError:\n",
    "        output_sentence = output_sentence + \" .\"\n",
    "    return output_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only character data\n",
    "\n",
    "def find_best_fit_word_char(input_word): ## \n",
    "    top_successor_tags = []\n",
    "    top_successor_words = []\n",
    "\n",
    "    char_tag_of_current_word = [word_tag_tuple[1] for (word_tag_tuple,frequency) in char_freqDist_World_word_grams.most_common() if word_tag_tuple[0] == input_word]\n",
    "\n",
    "    char_most_freq_successor_for_word = [wt[1] for (wt, _) in char_freqDist_word_grams.most_common() if wt[0] == input_word]\n",
    "\n",
    "    char_most_freq_successor_for_tag = [wt[1] for (wt, _) in char_freqDist_tag_grams.most_common() if wt[0] in char_tag_of_current_word]\n",
    "\n",
    "    _char_top_successor_words = char_most_freq_successor_for_word[:5]\n",
    "    _char_top_successor_tags = char_most_freq_successor_for_tag[:5]    \n",
    "    \n",
    "    top_successor_words = _char_top_successor_words\n",
    "    top_successor_tags = _char_top_successor_tags\n",
    "\n",
    "    \n",
    "    final_list = [(value,freq) for (value, freq) in freqDist_World_word_grams.most_common() if value[0] in top_successor_words and value[1] in top_successor_tags]\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_sentence_char(input_word):\n",
    "    import random\n",
    "    \n",
    "    output_sentence = \"\"\n",
    "\n",
    "    output_sentence += input_word\n",
    "    try:\n",
    "        best_fit_gram = find_best_fit_word_char(input_word)\n",
    "        best_fit_choice = random.choice(best_fit_gram)\n",
    "        best_fit_word = best_fit_choice[0][0]\n",
    "        output_sentence = output_sentence + \" \" + best_fit_word\n",
    "        while best_fit_word != '.':\n",
    "            best_fit_gram = find_best_fit_word_char(best_fit_word)\n",
    "            best_fit_choice = random.choice(best_fit_gram)\n",
    "            best_fit_word = best_fit_choice[0][0]\n",
    "            output_sentence = output_sentence + \" \" + best_fit_word\n",
    "    except IndexError:\n",
    "        output_sentence = output_sentence + \" .\"\n",
    "    return output_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \\n \\n Following is the output for Markov Chains (char and world data) --> \\n\\n\")\n",
    "\n",
    "try:\n",
    "    print(\"If the input word is 'I'\")\n",
    "    for i in range(5):\n",
    "        print(produce_sentence(input_word = \"I\"))\n",
    "\n",
    "\n",
    "    print(\"If the input word is 'And'\")\n",
    "    for i in range(5):\n",
    "        print(produce_sentence(input_word = \"And\"))\n",
    "\n",
    "\n",
    "    print(\"If the input word is 'Hey'\")\n",
    "    for i in range(5):\n",
    "        print(produce_sentence(input_word = \"Hey\"))\n",
    "\n",
    "\n",
    "    print(\"If the input word is 'The'\")\n",
    "    for i in range(5):\n",
    "        print(produce_sentence(input_word = \"The\"))\n",
    "\n",
    "\n",
    "    print(\"If the input word is 'You'\")\n",
    "    for i in range(5):\n",
    "        print(produce_sentence(input_word = \"You\"))\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \\n \\n Following is the output for Markov Chains only char data--> \\n\\n\")\n",
    "\n",
    "try:\n",
    "    print(\"If the input word is 'I'\")\n",
    "    for i in range(5):\n",
    "        print(produce_sentence_char(input_word = \"I\"))\n",
    "\n",
    "\n",
    "    print(\"If the input word is 'And'\")\n",
    "    for i in range(5):\n",
    "        print(produce_sentence_char(input_word = \"And\"))\n",
    "\n",
    "\n",
    "    print(\"If the input word is 'Hey'\")\n",
    "    for i in range(5):\n",
    "        print(produce_sentence_char(input_word = \"Hey\"))\n",
    "\n",
    "\n",
    "    print(\"If the input word is 'The'\")\n",
    "    for i in range(5):\n",
    "        print(produce_sentence_char(input_word = \"The\"))\n",
    "\n",
    "\n",
    "    print(\"If the input word is 'You'\")\n",
    "    for i in range(5):\n",
    "        print(produce_sentence_char(input_word = \"You\"))\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bigram Predictions without the use of POS tags\n",
    "\n",
    "def best_fit_bigram_prediction(input_word):\n",
    "    top_successor_tags = []\n",
    "    top_successor_words = []\n",
    "    most_freq_successor_for_word = [wt[1] for (wt, _) in freqDist_word_grams.most_common() if wt[0] == input_word]\n",
    "    char_most_freq_successor_for_word = [wt[1] for (wt, _) in char_freqDist_word_grams.most_common() if wt[0] == input_word]\n",
    "    \n",
    "    _top_successor_words = most_freq_successor_for_word[:5]\n",
    "    _char_top_successor_words = char_most_freq_successor_for_word[:5]  \n",
    "    if not _char_top_successor_words:\n",
    "        top_successor_words = _top_successor_words\n",
    "    \n",
    "    else:\n",
    "        top_successor_words = list(set(_top_successor_words + _char_top_successor_words))\n",
    "    \n",
    "    return top_successor_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_sentence_from_bigrams(input_word):\n",
    "    import random\n",
    "    \n",
    "    output_sentence = \"\"\n",
    "\n",
    "    output_sentence += input_word\n",
    "    try:\n",
    "        best_fit_gram = best_fit_bigram_prediction(input_word)\n",
    "        best_fit_choice = random.choice(best_fit_gram)\n",
    "        x = best_fit_bigram_prediction(input_word)\n",
    "        best_fit_word = best_fit_choice\n",
    "        output_sentence = output_sentence + \" \" + best_fit_word\n",
    "        while best_fit_word != '.':\n",
    "\n",
    "            best_fit_gram = best_fit_bigram_prediction(best_fit_word)\n",
    "            best_fit_choice = random.choice(best_fit_gram)\n",
    "            best_fit_word = best_fit_choice\n",
    "            output_sentence = output_sentence + \" \" + best_fit_word\n",
    "    except IndexError:\n",
    "        output_sentence = output_sentence + \" .\"\n",
    "    return output_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\" \\n \\n Following is the output for Bigram  probabilities --> \\n\\n\")\n",
    "\n",
    "print(\"If the input word is 'I'\")\n",
    "for i in range(10):\n",
    "    print(produce_sentence_from_bigrams(input_word = \"I\"))\n",
    "\n",
    "\n",
    "print(\"If the input word is 'And'\")\n",
    "for i in range(10):\n",
    "    print(produce_sentence_from_bigrams(input_word = \"And\"))\n",
    "\n",
    "\n",
    "print(\"If the input word is 'Hey'\")\n",
    "for i in range(10):\n",
    "    print(produce_sentence_from_bigrams(input_word = \"Hey\"))\n",
    "\n",
    "\n",
    "print(\"If the input word is 'The'\")\n",
    "for i in range(10):\n",
    "    print(produce_sentence_from_bigrams(input_word = \"The\"))\n",
    "\n",
    "\n",
    "print(\"If the input word is 'You'\")\n",
    "for i in range(10):\n",
    "    print(produce_sentence_from_bigrams(input_word = \"You\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## <<<<<<<<<<<<<<<<<<<<<<<< END of CODE >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
